#!/usr/bin/env python3
#
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.


import argparse
import contextlib
import datetime
import json
import logging
import os
import pathlib
import re
import shlex
import shutil
import subprocess
import sys
import tempfile
import threading
import time
import uuid

import ninja_syntax

from lib import litani, litani_report


################################################################################
# Argument parsing
################################################################################


def get_add_job_args():
    return [(
        "describing the build graph", [{
            "flags": ["-i", "--inputs"],
            "nargs": "+",
            "metavar": "F",
            "help": "list of files that this job depends on",
        }, {
            "flags": ["-c", "--command"],
            "metavar": "C",
            "required": True,
            "help": "the command to run once all dependencies are satisfied",
        }, {
            "flags": ["-o", "--outputs"],
            "metavar": "F",
            "nargs": "+",
            "help": "list of files that this job generates",
        }]), (
        "job control", [{
            "flags": ["-p", "--pipeline-name"],
            "required": True,
            "metavar": "P",
            "help": "which pipeline this job is a member of",
        }, {
            "flags": ["-s", "--ci-stage"],
            "required": True,
            "metavar": "S",
            "choices": litani.CI_STAGES,
            "help": "which CI stage this job should execute in. "
                    "Must be one of {%(choices)s}.",
        }, {
            "flags": ["--stats-group"],
            "metavar": "GROUP",
            "help": "which statistics report group this job is in"
        }, {
            "flags": ["--cwd"],
            "metavar": "DIR",
            "help": "directory that this job should execute in"
        }, {
            "flags": ["--timeout"],
            "metavar": "N",
            "type": int,
            "help": "max number of seconds this job should run for"
        }, {
            "flags": ["--timeout-ok"],
            "action": "store_true",
            "help": "if the job times out, terminate it and return success"
        }, {
            "flags": ["--ok-returns"],
            "metavar": "RC",
            "nargs": "+",
            "help": "if the job exits with one of the listed return codes, "
                    "return success"
        }, {
            "flags": ["--interleave-stdout-stderr"],
            "action": "store_true",
            "help": "simulate '2>&1 >...'"
        }, {
            "flags": ["--stdout-file"],
            "metavar": "FILE",
            "help": "file to redirect stdout to"
        }, {
            "flags": ["--stderr-file"],
            "metavar": "FILE",
            "help": "file to redirect stderr to"
        }]), (
        "misc", [{
            "flags": ["--description"],
            "metavar": "DESC",
            "help": "string to print when this job is being run",
        }]),
    ]


def get_exec_job_args():
    exec_job_args = list(get_add_job_args())
    exec_job_args.append((
        "`litani exec`-specific flags", [{
            "flags": ["--status-file"],
            "metavar": "F",
            "required": True,
            "help": "JSON file to write command status to",
    }, {
            "flags": ["--job-id"],
            "metavar": "ID",
            "required": True,
            "help": "the globally unique job ID",
    }]))
    return exec_job_args


def get_args():
    description = "Incrementally build up a dependency graph of jobs to execute"

    pars = argparse.ArgumentParser(description=description)
    subs = pars.add_subparsers(
        title="subcommands", dest="subcommand")
    subs.required = True

    for arg in [{
            "flags": ["-v", "--verbose"],
            "action": "store_true",
            "help": "verbose output",
        }, {
            "flags": ["-w", "--very-verbose"],
            "action": "store_true",
            "help": "very verbose output",
    }]:
        flags = arg.pop("flags")
        pars.add_argument(*flags, **arg)

    run_build_pars = subs.add_parser("init")
    run_build_pars.set_defaults(func=init)
    for arg in [{
            "flags": ["--project-name"],
            "required": True,
            "help": "Project that this proof run is associated with",
            "metavar": "NAME",
    }]:
        flags = arg.pop("flags")
        run_build_pars.add_argument(*flags, **arg)

    run_build_pars = subs.add_parser("run-build")
    run_build_pars.set_defaults(func=run_build)
    for arg in [{
            "flags": ["-n", "--dry-run"],
            "help": "don't actually run jobs, just act like they succeeded",
            "action": "store_true",
    }, {
            "flags": ["-j", "--parallel"],
            "metavar": "N",
            "help": "run N jobs in parallel. 0 means infinity, default is "
                    "based on the number of cores on this system.",
    }]:
        flags = arg.pop("flags")
        run_build_pars.add_argument(*flags, **arg)

    add_job_pars = subs.add_parser("add-job")
    add_job_pars.set_defaults(func=add_job)
    for group_name, args in get_add_job_args():
        group = add_job_pars.add_argument_group(title=group_name)
        for arg in args:
            flags = arg.pop("flags")
            group.add_argument(*flags, **arg)

    exec_job_pars = subs.add_parser("exec")
    exec_job_pars.set_defaults(func=exec_job)
    for group_name, args in get_exec_job_args():
        group = exec_job_pars.add_argument_group(title=group_name)
        for arg in args:
            flags = arg.pop("flags")
            group.add_argument(*flags, **arg)

    all_args = sys.argv[1:]
    wrapped_command = None
    if "--" in all_args:
        sep_idx = all_args.index("--")
        arg_list = all_args[0:sep_idx]
        wrapped_command = arg_list[sep_idx+1:]
        all_args = arg_list

    args = pars.parse_args(all_args)
    if wrapped_command is not None:
        args.command = wrapped_command

    return args


################################################################################
# Other utility functions
################################################################################


def render_report(cache_dir, report_dir):
    run = litani_report.render(cache_dir)
    with atomic_write(report_dir / "index.html") as handle:
        print(run, file=handle)


def continuous_render_report(cache_dir, report_dir, killer):
    while True:
        render_report(cache_dir, report_dir)
        if killer.is_set():
            break
        time.sleep(2)


@contextlib.contextmanager
def atomic_write(path, mode="w"):
    try:
        parent = pathlib.Path(path).parent
        parent.mkdir(exist_ok=True, parents=True)
        tmp = "%s~" % path
        handle = open(tmp, mode)
        yield handle
    except RuntimeError:
        try:
            os.unlink(tmp)
        except RuntimeError:
            pass
    else:
        handle.flush()
        handle.close()
        os.rename(tmp, path)


def set_up_logging(args):
    if args.very_verbose or args.verbose:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(
        format="litani:%(levelname)s: %(message)s", level=level)


def positive_int(arg):
    try:
        ret = int(arg)
    except ValueError:
        raise argparse.ArgumentTypeError("Timeout '%s' must be an int" % arg)
    if ret <= 0:
        raise argparse.ArgumentTypeError("Timeout '%d' must be > 0" % ret)
    return ret


def list_of_ints(arg):
    ret = []
    for rc in arg:
        try:
            ret.append(int(rc))
        except ValueError:
            raise argparse.ArgumentTypeError(
                "Return code '%s' must be an int" % arg)
    return ret


def timestamp(key, data):
    now = datetime.datetime.utcnow()
    data[key] = now.strftime("%Y-%m-%dT%H:%M:%SZ")


def make_litani_exec_command(add_args, status_file):
    cmd = [os.path.realpath(__file__), "exec"]
    # strings
    for arg in [
            "command", "pipeline_name", "ci_stage", "cwd", "job_id",
            "stdout_file", "stderr_file", "description", "timeout"
    ]:
        if arg in add_args:
            cmd.append("--%s" % arg.replace("_", "-"))
            cmd.append(shlex.quote(add_args[arg].strip()))

    # lists
    for arg in ["inputs", "outputs", "ok_returns"]:
        if arg not in add_args or add_args[arg] is None:
            continue
        cmd.append("--%s" % arg.replace("_", "-"))
        for item in add_args[arg]:
            cmd.append(shlex.quote(item.strip()))

    # switches
    for arg in [
            "timeout_ok", "interleave_stdout_stderr"
    ]:
        if arg in add_args:
            cmd.append("--%s" % arg.replace("_", "-"))

    cmd.extend(["--status-file", str(status_file)])

    return " ".join(cmd)


################################################################################
# Entry points
# ``````````````````````````````````````````````````````````````````````````````
# Each litani subcommand invokes one of these functions.
################################################################################


def init(args):
    run_id = str(uuid.uuid4())
    cache_dir = pathlib.Path(
        tempfile.gettempdir()) / "litani" / "runs" / run_id
    cache_dir.mkdir(parents=True)

    now = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

    with atomic_write(cache_dir / litani.CACHE_FILE) as handle:
        print(json.dumps({
            "project": args.project_name,
            "run_id": run_id,
            "start_time": now,
            "status": "in progress",
            "jobs": []
        }, indent=2), file=handle)

    logging.info("litani cache dir is at: %s", cache_dir)

    with atomic_write(litani.CACHE_POINTER) as handle:
        print(str(cache_dir), file=handle)


def add_job(args):
    litani_cache_dir = litani.get_cache_dir()

    with open(litani_cache_dir / litani.CACHE_FILE) as handle:
        cache = json.load(handle)

    args_dict = vars(args)
    args_dict.pop("func")
    for switch in list(args_dict.keys()):
        if not args_dict[switch]:
            args_dict.pop(switch)

    args_dict["job_id"] = str(uuid.uuid4())

    logging.debug("Adding job: %s", json.dumps(args_dict, indent=2))
    cache["jobs"].append(args_dict)

    with atomic_write(litani_cache_dir / litani.CACHE_FILE) as handle:
        json.dump(cache, handle, indent=2)


def run_build(args):
    cache_dir = litani.get_cache_dir()

    with open(cache_dir / litani.CACHE_FILE) as handle:
        cache = json.load(handle)

    rules = []
    builds = []
    status_dir = litani.get_status_dir()
    for entry in cache["jobs"]:

        if "description" in entry:
            description = entry["description"]
        else:
            description = f"Running '{entry['command']}...'"

        rule_name = entry["job_id"]
        status_file = status_dir / ("%s.json" % rule_name)

        rules.append({
            "name": rule_name,
            "description": description,
            "command": make_litani_exec_command(entry, status_file)
        })

        builds.append({
            "inputs": entry["inputs"],
            "outputs": entry["outputs"] + [str(status_file)],
            "rule": rule_name
        })

    ninja_file = cache_dir / "litani.ninja"
    with atomic_write(ninja_file) as handle:
        ninja = ninja_syntax.Writer(handle, width=70)
        for rule in rules:
            logging.debug(rule)
            ninja.rule(**rule)
        for build in builds:
            logging.debug(build)
            ninja.build(**build)

    report_dir = litani.get_report_dir()
    render_report(cache_dir, report_dir)
    logging.info("Report will be rendered at file://%s/index.html", str(report_dir))
    killer = threading.Event()
    render_thread = threading.Thread(
        group=None, target=continuous_render_report,
        args=(cache_dir, report_dir, killer))
    render_thread.start()

    cmd = [
        "ninja",
        "-k", "0",
        "-f", ninja_file,
    ]
    if args.parallel:
        cmd.extend(["-j", args.parallel])
    if args.dry_run:
        cmd.append("-n")
    proc = subprocess.run(cmd)

    now = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

    with open(cache_dir / litani.CACHE_FILE) as handle:
        run_info = json.load(handle)
    run_info["end_time"] = now

    success = True
    for root, _, files in os.walk(str(status_dir)):
        for fyle in files:
            if not fyle.endswith(".json"):
                continue
            with open(os.path.join(root, fyle)) as handle:
                job_status = json.load(handle)
            if job_status["command-return-code"]:
                success = False
    run_info["status"] = "success" if success else "failure"

    with atomic_write(cache_dir / litani.CACHE_FILE) as handle:
        print(json.dumps(run_info, indent=2), file=handle)

    killer.set()
    render_thread.join()
    render_report(cache_dir, report_dir)

    shutil.copytree(litani.get_artifacts_dir(), report_dir / "artifacts")

    logging.info("Status files have been written to %s", str(status_dir))
    sys.exit(1 if proc.returncode else 0)


def exec_job(args):
    args_dict = vars(args)
    args_dict.pop("func")
    out_data = {
        "wrapper-arguments": args_dict,
        "wrapper-return-code": 0,
        "complete": False,
    }
    timestamp("start-time", out_data)
    with atomic_write(args.status_file) as handle:
        print(json.dumps(out_data, indent=2), file=handle)

    if args.interleave_stdout_stderr:
        stderr = subprocess.STDOUT
    else:
        stderr = subprocess.PIPE

    proc = subprocess.Popen(
        args=args.command, shell=True, universal_newlines=True,
        stdout=subprocess.PIPE, stderr=stderr, cwd=args.cwd)

    try:
        proc_out, proc_err = proc.communicate(timeout=args.timeout)
    except subprocess.TimeoutExpired:
        proc.kill()
        proc_out, proc_err = proc.communicate()
        out_data["timeout-reached"] = True
        if not args.timeout_ok:
            out_data["wrapper-return-code"] = 1
    else:
        out_data["timeout-reached"] = False

    out_data["command-return-code"] = proc.returncode
    out_data["complete"] = True
    ok_returns = args.ok_returns if args.ok_returns else []
    ok_returns = [int(i) for i in ok_returns]
    ok_returns.append(0)
    if proc.returncode not in ok_returns:
        out_data["wrapper-return-code"] = 1

    for out_field, proc_pipe, arg_file, add_to_out in [
        ("stdout", proc_out, args.stdout_file, True),
        ("stderr", proc_err, args.stderr_file, args.interleave_stdout_stderr)
    ]:
        if add_to_out and proc_pipe:
            out_data[out_field] = proc_pipe.splitlines()
        else:
            out_data[out_field] = []
        if arg_file:
            with atomic_write(arg_file) as handle:
                print(proc_pipe, file=handle)

    timestamp("end-time", out_data)
    out_str = json.dumps(out_data, indent=2)
    logging.debug(out_str)
    with atomic_write(args.status_file) as handle:
        print(out_str, file=handle)

    artifacts_dir = (litani.get_artifacts_dir() /
         out_data["wrapper-arguments"]["pipeline_name"] /
         out_data["wrapper-arguments"]["ci_stage"])
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    for fyle in out_data["wrapper-arguments"]["outputs"]:
        try:
            shutil.copy(fyle, str(artifacts_dir))
        except FileNotFoundError:
            logging.warning(
                "Output file '%s' of pipeline '%s' did not exist upon job "
                "completion. Not copying to artifacts directory.", fyle,
            out_data["wrapper-arguments"]["pipeline_name"])
        except IsADirectoryError:
            artifact_src = pathlib.Path(fyle)
            shutil.copytree(fyle, str(artifacts_dir / artifact_src.name))

    if out_data["wrapper-return-code"] == 0:
        sys.exit(0)

    #logging.error(" ".join(out_data["stdout"]))
    #if not args.interleave_stdout_stderr:
    #    logging.error(" ".join(out_data["stderr"]))
    #logging.error(
    #    "Further details about this failed job are recorded in %s",
    #    args.status_file)
    sys.exit(out_data["wrapper-return-code"])


def main():
    args = get_args()
    set_up_logging(args)
    args.func(args)


if __name__ == "__main__":
    main()
